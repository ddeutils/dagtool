import os
from datetime import datetime, timedelta
from functools import partial
from pathlib import Path
from typing import Any, Literal, Union

from airflow.models import DAG
from pydantic import BaseModel, Field, ValidationError
from yaml import safe_load
from yaml.parser import ParserError

from .const import ASSET_DIR, VARIABLE_FILENAME
from .plugins.tasks import AnyTask
from .utils import TaskMapped, set_upstream


class DefaultArgs(BaseModel):
    """Default Args Model that will use with the `default_args` field."""

    owner: str | None = None
    depends_on_past: bool = False

    def to_dict(self) -> dict[str, Any]:
        return self.model_dump(exclude_defaults=True)


class DagModel(BaseModel):
    """Base Dag Model for validate template config data support DagTool object.
    This model will include necessary field for Airflow DAG object and custom
    field for DagTool object together.
    """

    name: str = Field(description="A DAG name.")
    type: Literal["dag"] = Field(description="A type of template config.")
    docs: str | None = Field(
        default=None,
        description="A DAG document that allow to pass with markdown syntax.",
    )
    params: dict[str, str] = Field(default_factory=dict)
    tasks: list[AnyTask] = Field(
        default_factory=list,
        description="A list of any task, origin task or group task",
    )

    # NOTE: Runtime parameters that extract from YAML loader step.
    filename: str | None = Field(
        default=None,
        description="A filename of the current position.",
    )
    parent_dir: Path | None = Field(
        default=None, description="A parent dir path."
    )
    created_dt: datetime | None = Field(
        default=None, description="A file created datetime."
    )
    updated_dt: datetime | None = Field(
        default=None, description="A file modified datetime."
    )
    raw_data: str | None = Field(default=None)

    # NOTE: Airflow DAG parameters.
    owner: str = Field(default="dagtool")
    tags: list[str] = Field(default_factory=list, description="A list of tags.")
    schedule: str | None = Field(default=None)
    schedule_interval: str | None = Field(default=None)
    start_date: str | None = Field(default=None)
    end_date: str | None = Field(default=None)
    concurrency: int | None = Field(default=None)
    max_active_runs: int = Field(default=1)
    dagrun_timeout_sec: int = Field(default=600)
    default_args: DefaultArgs = Field(default_factory=DefaultArgs)

    def create_docs(self, docs: str | None = None):
        """Generated document string that merge between parent docs and template
        docs together.

        Args:
            docs (str): A parent documents
        """
        if docs:
            d: str = docs.rstrip("\n")
            docs: str = f"{d}\n\n{self.docs}\nGenerated by DAG Tools."

        # TODO: Exclude jinja template until upgrade Airflow >= 2.9.3
        raw_data = self.raw_data.replace(
            "{{", "{% raw %}{{{% endraw %}"
        ).replace("}}", "{% raw %}}}{% endraw %}")

        if docs:
            docs += f"\n\n## YAML Template\n\n```yaml\n{raw_data}\n```"
        else:
            docs += f"## YAML Template\n\n```yaml\n{raw_data}\n```"
        return docs

    def build(
        self,
        prefix: str | None,
        docs: str | None = None,
        default_args: dict[str, Any] | None = None,
        user_defined_macros: dict[str, Any] | None = None,
        template_searchpath: list[str] | None = None,
    ) -> DAG:
        """Build Airflow DAG object from the current model field values.

        Args:
            prefix (str | None): A prefix of DAG name.
            docs (str | None): A document string with Markdown syntax.
            default_args: (dict[str, Any]): An override default arguments to the
                Airflow DAG object.
            user_defined_macros:
            template_searchpath:
        """
        name: str = f"{prefix}_{self.name}" if prefix else self.name
        dag = DAG(
            dag_id=name,
            tags=self.tags,
            doc_md=self.create_docs(docs),
            schedule=self.schedule,
            start_date=self.start_date,
            end_date=self.end_date,
            concurrency=self.concurrency,
            max_active_runs=self.max_active_runs,
            dagrun_timeout=timedelta(seconds=self.dagrun_timeout_sec),
            default_args={
                "owner": self.owner,
                **(default_args or {}),
            },
            template_searchpath=[str((self.parent_dir / ASSET_DIR).absolute())]
            + (template_searchpath or []),
            user_defined_macros={
                "var": partial(
                    get_variable_stage(self.parent_dir, name=self.name).get
                ),
            }
            | (user_defined_macros or {}),
            user_defined_filters={},
        )
        tasks: dict[str, TaskMapped] = {}
        for task in self.tasks:
            tasks[task.iden] = {
                "upstream": task.upstream,
                "task": task.build(task_group=None, dag=dag),
            }

        # NOTE: Set upstream for each task.
        set_upstream(tasks)

        return dag


Primitive = Union[str, int, float, bool]
ValueType = Union[Primitive, list[Primitive], dict[Union[str, int], Primitive]]


class Key(BaseModel):
    """Key Model."""

    key: str = Field(description="A key name.")
    desc: str | None = Field(
        default=None,
        description="A description of this variable.",
    )
    stages: dict[str, dict[str, ValueType]] = Field(
        default=dict,
        description="A stage mapping with environment and its pair of variable",
    )


class Variable(BaseModel):
    """Variable Model."""

    type: Literal["variable"] = Field(description="A type of variable model.")
    variables: list[Key] = Field(description="A list of Key model.")

    def get_key(self, name: str) -> Key:
        """Get the Key model with an input specific key name.

        Args:
            name (str): A key name.

        Returns:
            Key: A Key model.
        """
        for k in self.variables:
            if name == k.key:
                return k
        raise ValueError(f"A key: {name} does not set on this variables.")


def read_variable(key: str, path: Path, name: str) -> dict[str, Any]:
    """Get Variable value with an input stage name."""
    search_files: list[Path] = list(path.rglob(f"{VARIABLE_FILENAME}.y*ml"))
    if not search_files:
        raise FileNotFoundError("Does not found variables file.")
    try:
        var = Variable.model_validate(
            safe_load(search_files[0].open(mode="rt"))
        )
        return (
            var.get_key(name)
            .stages.get(os.getenv("AIRFLOW_ENV", "NOTSET"), {})
            .get(key, None)
        )
    except ParserError:
        raise
    except ValidationError:
        raise


def get_variable_stage(path: Path, name: str) -> dict[str, Any]:
    search_files: list[Path] = list(path.rglob(f"{VARIABLE_FILENAME}.y*ml"))
    if not search_files:
        return {}
    try:
        var = Variable.model_validate(
            safe_load(search_files[0].open(mode="rt"))
        )
        return var.get_key(name).stages.get(
            os.getenv("AIRFLOW_ENV", "NOTSET"), {}
        )
    except ParserError:
        raise
    except ValidationError:
        raise
    except ValueError:
        return {}
